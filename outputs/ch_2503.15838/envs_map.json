[
    {
        "placeholder": "<PLACEHOLDER_ENV_1>",
        "env_name": "abstract",
        "content": "\\begin{abstract}\nEnsemble learning has been widely used in machine learning to improve model robustness, accuracy, and generalization, but has not yet been applied to code generation tasks with large language\nmodels (LLMs). We propose an ensemble approach for LLMs in code generation. Instead of relying on the output of a single model, we generate multiple candidate programs from different LLMs and apply a structured voting mechanism to select the most reliable solution.\nFor voting, we compute syntactic and semantic similarity using CodeBLEU and behavioral equivalence using CrossHair’s differential behavior analysis. By aggregating these similarity scores, we select the program that best aligns with the consensus among the candidates. We show through experiments that our ensemble approach consistently outperforms standalone\nLLMs on the well-known HumanEval and the more challenging LiveCodeBench datasets, achieving an accuracy of 90.2\\% and 50.2\\%, respectively, on the two datasets. In comparison, the best-performing LLM (GPT-4o) has an accuracy of 83.5\\% and 43.4\\%, respectively.\nFurthermore, even when restricted to free open source models, our method achieves an accuracy of 80.5\\%  and 41.6\\%, respectively,  demonstrating the viability of our approach in resource-constrained settings.\n\\end{abstract}",
        "trans_content": "\\begin{abstract}\n集成学习在机器学习领域已被广泛用于提升模型的鲁棒性、准确性和泛化能力，但尚未应用于基于大语言模型（LLMs）的代码生成任务。我们提出了一种面向LLM代码生成的集成方法。该方法不依赖单一模型的输出，而是从不同LLM生成多个候选程序，并通过结构化投票机制选择最可靠的解决方案。投票过程中，我们使用CodeBLEU计算语法与语义相似度，并利用CrossHair的差异化行为分析评估行为等价性。通过聚合这些相似度评分，我们选择与候选程序共识最匹配的方案。实验表明，在知名HumanEval数据集和更具挑战性的LiveCodeBench数据集上，我们的集成方法始终优于单一LLM，分别达到90.2\\%和50.2\\%的准确率。相比之下，性能最佳的单一LLM（GPT-4o）准确率分别为83.5\\%和43.4\\%。此外，即使仅使用免费开源模型，我们的方法仍能分别实现80.5\\%和41.6\\%的准确率，证明了该方法在资源受限场景下的可行性。\n\\end{abstract}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_2>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item We propose an ensemble-based approach for LLM-based code generation, leveraging syntactic, semantic, and behavioral similarity for improved reliability.\n    \\item We propose a novel voting mechanism that integrates CodeBLEU and CrossHair to assess syntactic similarity, semantic alignment, and functional correctness of generated program.\n    \\item We conduct extensive experiments on HumanEval and LiveCodeBench, demonstrating that \\tool\\ significantly outperforms standalone LLMs. We analyze failure cases and demonstrate that correct programs reinforce each other, supporting our assumption that independent LLMs are less prone to making identical mistakes.\n    \\item We show that \\tool\\ remains effective even when restricted to free and open-source LLMs, making it a viable solution for environments with limited access to proprietary models.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_3>",
        "env_name": "figure*",
        "content": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figure/EnsembledLLM.pdf}\n    <PLACEHOLDER_CAP_3>\n    \\label{fig:overview}\n\\end{figure*}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_4>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item \\textbf{Syntactic Structure Matching (syntax\\_weight):}\nUses Abstract Syntax Tree (AST) matching to compare the syntactical organization of programs. programs with similar syntax trees are syntactically equivalent, even if their tokens differ. This prevents the penalization of programs with different formatting or minor syntactical variations.\n    \\item \\textbf{Semantic Data-Flow Matching (dataflow\\_weight):}\nAnalyzes data-flow dependencies between variables to assess functional similarity. Two programs are considered equivalent if they manipulate variables and data in the same way, regardless of syntax. This ensures that semantically identical but lexically different implementations receive high similarity scores.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_5>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Iterative Binary Search Implementation}, label={lst:binary_search_iterative}]\ndef binary_search_iterative(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\\end{lstlisting}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_6>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Recursive Binary Search Implementation}, label={lst:binary_search_recursive}]\ndef binary_search_recursive(arr, target, left, right):\n    if left > right:\n        return -1\n    mid = (left + right) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] < target:\n        return binary_search_recursive(arr,\n                            target, mid + 1, right)\n    else:\n        return binary_search_recursive(arr,\n                            target, left, mid - 1)\n\ndef binary_search(arr, target):\n    return binary_search_recursive(arr,\n                            target, 0, len(arr) - 1)\n\\end{lstlisting}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_7>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Linear Search Implementation (Incorrect, as the task requires a Binary Search algorithm)}, label={lst:linear_search}]\ndef linear_search(arr, target):\n    for i in range(len(arr)):\n        if arr[i] == target:\n            return i\n    return -1\n\\end{lstlisting}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_8>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item Zero counterexamples indicate that two programs are behaviorally identical, meaning they produce the same output for all tested inputs.\n    \\item One or more counterexamples indicate that the programs exhibit different behaviors, highlighting logic errors or functional differences.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_9>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\label{eq:beh_similarity}\n\\begin{split}\n    \\text{BSim}_n(P_i, P_j) &=  \\left(1 - \\frac{\\text{MIN}(n, ({\\text{cex}}(P_i, P_j))}{n} \\right)\n\\end{split}\n\\end{equation}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_10>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Incorrect Recursive Binary Search Implementation}, label={lst:binary_search_recursive_wrong}]\ndef binary_search_recursive_wrong(arr, target, left, right):\n    if left >= right:\n        return -1\n    mid = left + (right - left) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] < target:\n        return binary_search_recursive_wrong(arr,\n                                    target, mid, right)\n    else:\n        return binary_search_recursive_wrong(arr,\n                                    target, left, mid)\n\\end{lstlisting}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_11>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\label{eq:similarity}\n\\begin{split}\n   \\text{similarity}(P_i, P_j) &= \\lambda \\cdot \\text{CodeBLEU}(P_i, P_j) + (1 - \\lambda) \\cdot \\text{BSim}_n(P_i, P_j)\n\\end{split}\n\\end{equation}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_12>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\text{aggregated\\_similarity}(P_i) = \\sum_{j \\neq i} \\text{similarity}(P_i, P_j)\n\\end{equation}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_13>",
        "env_name": "algorithm",
        "content": "\\begin{algorithm}[t!]\n<PLACEHOLDER_CAP_4>\n\\label{alg:ensemble_selection}\n\\begin{algorithmic}[1]\n\n\\Require List of programs \\( P = \\{P_1, P_2, ..., P_n\\} \\)\n\\Ensure The program \\( P^* \\) with the highest aggregated similarity\n\n\\For{each \\( P_i \\) in \\( P \\)}\n    \\For{each \\( P_j \\) in \\( P \\) where \\( i \\neq j \\)}\n        \\State \\( \\text{similarity}[i][j] \\gets \\text{Compute\\_Similarity}(P_i, P_j) \\)\n    \\EndFor\n\\EndFor\n\n\\For{each \\( P_i \\) in \\( P \\)}\n    \\State \\( \\text{agg\\_sim}[i] \\gets 0 \\)\n    \\For{each \\( P_j \\) in \\( P \\) where \\( i \\neq j \\)}\n        \\State \\( \\text{agg\\_sim}[i] \\gets \\text{agg\\_sim}[i] + \\text{similarity}[i][j] \\)\n    \\EndFor\n\\EndFor\n\n\\State \\( \\text{best\\_programs} \\gets \\{ P_i \\mid \\text{agg\\_sim}[i] = \\max(\\text{agg\\_sim}) \\} \\)\n\n\\State \\( P^* \\gets tie\\_breaking(\\text{best\\_programs}) \\)\n\n\\State \\Return \\( P^* \\)\n\n\\end{algorithmic}\n\\end{algorithm}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_14>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item\n{\\textbf{RQ1: How does \\tool\\ perform compared to standalone LLMs?}}\n    \\\\ We measure functional correctness using pass@1 metrics and evaluate if \\tool\\ improves the overall success rate over standalone models.\n    \\item\n{\\textbf {RQ2: How do CodeBLEU and CrossHair individually contribute to the accuracy of \\tool?}}\n    \\\\ An ablation study is conducted to assess the independent impact of CodeBLEU (syntactical and semantical similarity) and CrossHair (behavioral equivalence testing) in selecting correct outputs.\n    \\item\n{\\textbf{RQ3: How much accuracy can be gained using only free LLMs?}}\n    \\\\ We analyze whether \\tool\\ can effectively leverage only open-source models (e.g., OpenChat, CodeLlama, DeepSeekCoder) while maintaining high accuracy.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_15>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item \\textbf{HumanEval} \\cite{humaneval} is a benchmark specifically designed for evaluating the functional correctness of LLM-generated code. It consists of 164 Python programming problems, each presented with a structured format that includes a natural language prompt, a function signature, and a set of hidden test cases used for verification. The problems span a variety of domains, including mathematical computations, string manipulations, data structures, and logical reasoning. Since the primary focus of HumanEval is on correctness rather than efficiency or complexity, the generated program is evaluated using the pass@k metric. This metric determines whether at least one of the top k generated solutions is functionally correct based on the provided test cases. Given its controlled setup and standardized evaluation framework, HumanEval has become a widely adopted benchmark in the automated code generation community.\n\n    \\item \\textbf{LiveCodeBench} \\cite{livecodebench} is a comprehensive benchmark designed to assess the capabilities of LLMs across multiple programming related tasks. The dataset consists of four distinct categories: Code Generation, Self-Repair, Test Output Prediction, and Code Execution. For our evaluation, we focus exclusively on the Code Generation subset, which contains 511 programming problems covering a wide range of coding scenarios. These problems are more diverse than those in HumanEval, as they include real-world coding tasks, API usage, algorithmic challenges, and data structure manipulations. Additionally, LiveCodeBench introduces more complex problem formulations, requiring models to not only generate syntactically correct code but also ensure proper API usage and logical coherence. Like HumanEval, pass@k is used as the primary evaluation metric, providing a robust way to measure functional correctness. The inclusion of this dataset ensures that \\tool\\ is tested on a broader range of coding challenges that more accurately reflect real-world software development scenarios.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_16>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item Proprietary LLMs: GPT-4o, GPT-4, GPT-3.5, CoPilot, Gemini\n    \\item Open-Source LLMs: OpenChat, CodeBERT, Llama 3.2, Qwen2, Codestral, Gemma 2, DeepSeekCoder, CodeLlama, DolphinCoder\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_17>",
        "env_name": "table",
        "content": "\\begin{table}[t!]\n    \\centering\n    <PLACEHOLDER_CAP_5>\n    \\label{tab:elfcg_performance}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{LLM} & \\textbf{HumanEval (\\%)} & \\textbf{LiveCodeBench (\\%)} \\\\\n        \\hline\n        GPT-4o         & 83.5  & 43.4  \\\\\n        GPT-4          & 82.3  & 42.2  \\\\\n        GPT-3.5        & 76.8  & 39.1  \\\\\n        CoPilot        & 74.4  & 41.6  \\\\\n        OpenChat (F)      & 71.3  & 37.3  \\\\\n        CodeBert (F)      & 68.9  & 37.2  \\\\\n        Llama 3.2 (F)     & 68.9  & 36.7  \\\\\n        Gemini         & 66.5  & 36.1  \\\\\n        Qwen2 (F)         & 62.1  & 36.4  \\\\\n        Codestral (F)     & 59.2  & 37.2  \\\\\n        Gemma 2  (F)      & 52.1  & 31.3  \\\\\n        DeepSeekCoder (F)  & 50.6  & 25.6  \\\\\n        CodeLlama  (F)     & 42.7  & 23.4  \\\\\n        DolphinCoder (F)   & 34.8  & 22.2  \\\\\n        \\hline\n        \\textbf{\\tool\\ (All)} & \\textbf{90.2 (90.9)} & \\textbf{50.2 (53.8)} \\\\\n        \\hline\n        \\textbf{\\tool\\ (Top 5)} & \\textbf{87.2 (90.9)} & \\textbf{48.3 (53.8)} \\\\\n        \\hline\n        \\textbf{\\tool\\ (All Free)} & \\textbf{80.5 (83.2)} & \\textbf{41.6 (44.1)} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_18>",
        "env_name": "table",
        "content": "\\begin{table}[t!]\n    \\centering\n    <PLACEHOLDER_CAP_6>\n    \\label{tab:ablation_codebleu_crosshair}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{$\\lambda$} & \\textbf{HumanEval (\\%)} & \\textbf{LiveCodeBench (\\%)} \\\\\n        \\hline\n        1 & 85.9 & 49.5 \\\\\n        .75 & 87.2 & 49.5 \\\\\n        .5 & \\textbf{90.2} & \\textbf{50.2} \\\\\n        .25 & 89.6 & 48.4 \\\\\n        0 & 89.6 & 47.9 \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    }
]