[
    {
        "placeholder": "<PLACEHOLDER_ENV_1>",
        "env_name": "abstract",
        "content": "\\begin{abstract}\nEnsemble learning has been widely used in machine learning to improve model robustness, accuracy, and generalization, but has not yet been applied to code generation tasks with large language\nmodels (LLMs). We propose an ensemble approach for LLMs in code generation. Instead of relying on the output of a single model, we generate multiple candidate programs from different LLMs and apply a structured voting mechanism to select the most reliable solution.\nFor voting, we compute syntactic and semantic similarity using CodeBLEU and behavioral equivalence using CrossHair’s differential behavior analysis. By aggregating these similarity scores, we select the program that best aligns with the consensus among the candidates. We show through experiments that our ensemble approach consistently outperforms standalone\nLLMs on the well-known HumanEval and the more challenging LiveCodeBench datasets, achieving an accuracy of 90.2\\% and 50.2\\%, respectively, on the two datasets. In comparison, the best-performing LLM (GPT-4o) has an accuracy of 83.5\\% and 43.4\\%, respectively.\nFurthermore, even when restricted to free open source models, our method achieves an accuracy of 80.5\\%  and 41.6\\%, respectively,  demonstrating the viability of our approach in resource-constrained settings.\n\\end{abstract}",
        "trans_content": "\\begin{abstract}\n集成学习在机器学习领域已被广泛用于提升模型的鲁棒性、准确性和泛化能力，但尚未应用于基于大语言模型（LLMs）的代码生成任务。我们提出了一种面向LLMs代码生成的集成方法。该方法不依赖单一模型的输出，而是从不同LLM生成多个候选程序，并通过结构化投票机制选择最可靠的解决方案。投票过程中，我们使用CodeBLEU计算语法和语义相似度，并借助CrossHair的差异行为分析评估行为等价性。通过聚合这些相似度评分，我们选择与候选方案共识最契合的程序。实验表明，在知名HumanEval数据集和更具挑战性的LiveCodeBench数据集上，我们的集成方法始终优于单一LLM，分别达到90.2\\%和50.2\\%的准确率。相比之下，性能最佳的单一LLM（GPT-4o）准确率分别为83.5\\%和43.4\\%。此外，即使仅使用免费开源模型，我们的方法仍能分别实现80.5\\%和41.6\\%的准确率，证明了该方法在资源受限环境下的可行性。\n\\end{abstract}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_2>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item We propose an ensemble-based approach for LLM-based code generation, leveraging syntactic, semantic, and behavioral similarity for improved reliability.\n    \\item We propose a novel voting mechanism that integrates CodeBLEU and CrossHair to assess syntactic similarity, semantic alignment, and functional correctness of generated program.\n    \\item We conduct extensive experiments on HumanEval and LiveCodeBench, demonstrating that \\tool\\ significantly outperforms standalone LLMs. We analyze failure cases and demonstrate that correct programs reinforce each other, supporting our assumption that independent LLMs are less prone to making identical mistakes.\n    \\item We show that \\tool\\ remains effective even when restricted to free and open-source LLMs, making it a viable solution for environments with limited access to proprietary models.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}\n    \\item 我们提出一种基于集成学习的LLM代码生成方法，通过利用语法、语义和行为相似性来提升可靠性。\n    \\item 我们设计了一种新颖的投票机制，整合CodeBLEU和CrossHair指标来评估生成程序的语法相似性、语义对齐度和功能正确性。\n    \\item 在HumanEval和LiveCodeBench基准上进行了大量实验，证明\\tool\\显著优于单一LLM。通过分析失败案例，我们发现正确程序会相互增强，这支持了我们的假设：独立LLM不太可能犯相同错误。\n    \\item 实验表明，即使仅使用免费开源LLM，\\tool\\仍保持有效性，这使其成为无法使用商业模型环境下的可行解决方案。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_3>",
        "env_name": "figure*",
        "content": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figure/EnsembledLLM.pdf}\n    <PLACEHOLDER_CAP_3>\n    \\label{fig:overview}\n\\end{figure*}",
        "trans_content": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figure/EnsembledLLM.pdf}\n    <PLACEHOLDER_CAP_3>\n    \\label{fig:overview}\n\\end{figure*}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_4>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item \\textbf{Syntactic Structure Matching (syntax\\_weight):}\nUses Abstract Syntax Tree (AST) matching to compare the syntactical organization of programs. programs with similar syntax trees are syntactically equivalent, even if their tokens differ. This prevents the penalization of programs with different formatting or minor syntactical variations.\n    \\item \\textbf{Semantic Data-Flow Matching (dataflow\\_weight):}\nAnalyzes data-flow dependencies between variables to assess functional similarity. Two programs are considered equivalent if they manipulate variables and data in the same way, regardless of syntax. This ensures that semantically identical but lexically different implementations receive high similarity scores.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}\n    \\item \\textbf{语法结构匹配（syntax\\_weight）：}\n通过抽象语法树（AST）匹配比较程序的语法组织形式。具有相似语法树的程序在句法上是等价的，即使它们的标记不同。这避免了因格式差异或微小句法变体而对程序进行惩罚。\n    \\item \\textbf{语义数据流匹配（dataflow\\_weight）：}\n分析变量间的数据流依赖关系以评估功能相似性。若两个程序以相同方式操作变量和数据，则视为语义等价，与语法无关。这确保语义相同但词法不同的实现能获得高相似度评分。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_5>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Iterative Binary Search Implementation}, label={lst:binary_search_iterative}]\ndef binary_search_iterative(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\\end{lstlisting}",
        "trans_content": "\\begin{lstlisting}[language=Python, caption={Iterative Binary Search Implementation}, label={lst:binary_search_iterative}]\ndef binary_search_iterative(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\\end{lstlisting}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_6>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Recursive Binary Search Implementation}, label={lst:binary_search_recursive}]\ndef binary_search_recursive(arr, target, left, right):\n    if left > right:\n        return -1\n    mid = (left + right) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] < target:\n        return binary_search_recursive(arr,\n                            target, mid + 1, right)\n    else:\n        return binary_search_recursive(arr,\n                            target, left, mid - 1)\n\ndef binary_search(arr, target):\n    return binary_search_recursive(arr,\n                            target, 0, len(arr) - 1)\n\\end{lstlisting}",
        "trans_content": "\\begin{lstlisting}[language=Python, caption={Recursive Binary Search Implementation}, label={lst:binary_search_recursive}]\ndef binary_search_recursive(arr, target, left, right):\n    if left > right:\n        return -1\n    mid = (left + right) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] < target:\n        return binary_search_recursive(arr,\n                            target, mid + 1, right)\n    else:\n        return binary_search_recursive(arr,\n                            target, left, mid - 1)\n\ndef binary_search(arr, target):\n    return binary_search_recursive(arr,\n                            target, 0, len(arr) - 1)\n\\end{lstlisting}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_7>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Linear Search Implementation (Incorrect, as the task requires a Binary Search algorithm)}, label={lst:linear_search}]\ndef linear_search(arr, target):\n    for i in range(len(arr)):\n        if arr[i] == target:\n            return i\n    return -1\n\\end{lstlisting}",
        "trans_content": "\\begin{lstlisting}[language=Python, caption={Linear Search Implementation (Incorrect, as the task requires a Binary Search algorithm)}, label={lst:linear_search}]\ndef linear_search(arr, target):\n    for i in range(len(arr)):\n        if arr[i] == target:\n            return i\n    return -1\n\\end{lstlisting}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_8>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item Zero counterexamples indicate that two programs are behaviorally identical, meaning they produce the same output for all tested inputs.\n    \\item One or more counterexamples indicate that the programs exhibit different behaviors, highlighting logic errors or functional differences.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}\n    \\item 零反例表明两个程序在行为上完全一致，这意味着在所有测试输入下它们产生相同的输出。\n    \\item 一个或多个反例表明程序表现出不同的行为，突显了逻辑错误或功能差异。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_9>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\label{eq:beh_similarity}\n\\begin{split}\n    \\text{BSim}_n(P_i, P_j) &=  \\left(1 - \\frac{\\text{MIN}(n, ({\\text{cex}}(P_i, P_j))}{n} \\right)\n\\end{split}\n\\end{equation}",
        "trans_content": "\\begin{equation}\n\\label{eq:beh_similarity}\n\\begin{split}\n    \\text{BSim}_n(P_i, P_j) &=  \\left(1 - \\frac{\\text{MIN}(n, ({\\text{cex}}(P_i, P_j))}{n} \\right)\n\\end{split}\n\\end{equation}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_10>",
        "env_name": "lstlisting",
        "content": "\\begin{lstlisting}[language=Python, caption={Incorrect Recursive Binary Search Implementation}, label={lst:binary_search_recursive_wrong}]\ndef binary_search_recursive_wrong(arr, target, left, right):\n    if left >= right:\n        return -1\n    mid = left + (right - left) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] < target:\n        return binary_search_recursive_wrong(arr,\n                                    target, mid, right)\n    else:\n        return binary_search_recursive_wrong(arr,\n                                    target, left, mid)\n\\end{lstlisting}",
        "trans_content": "\\begin{lstlisting}[language=Python, caption={Incorrect Recursive Binary Search Implementation}, label={lst:binary_search_recursive_wrong}]\ndef binary_search_recursive_wrong(arr, target, left, right):\n    if left >= right:\n        return -1\n    mid = left + (right - left) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] < target:\n        return binary_search_recursive_wrong(arr,\n                                    target, mid, right)\n    else:\n        return binary_search_recursive_wrong(arr,\n                                    target, left, mid)\n\\end{lstlisting}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_11>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\label{eq:similarity}\n\\begin{split}\n   \\text{similarity}(P_i, P_j) &= \\lambda \\cdot \\text{CodeBLEU}(P_i, P_j) + (1 - \\lambda) \\cdot \\text{BSim}_n(P_i, P_j)\n\\end{split}\n\\end{equation}",
        "trans_content": "\\begin{equation}\n\\label{eq:similarity}\n\\begin{split}\n   \\text{similarity}(P_i, P_j) &= \\lambda \\cdot \\text{CodeBLEU}(P_i, P_j) + (1 - \\lambda) \\cdot \\text{BSim}_n(P_i, P_j)\n\\end{split}\n\\end{equation}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_12>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\text{aggregated\\_similarity}(P_i) = \\sum_{j \\neq i} \\text{similarity}(P_i, P_j)\n\\end{equation}",
        "trans_content": "\\begin{equation}\n\\text{aggregated\\_similarity}(P_i) = \\sum_{j \\neq i} \\text{similarity}(P_i, P_j)\n\\end{equation}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_13>",
        "env_name": "algorithm",
        "content": "\\begin{algorithm}[t!]\n<PLACEHOLDER_CAP_4>\n\\label{alg:ensemble_selection}\n\\begin{algorithmic}[1]\n\n\\Require List of programs \\( P = \\{P_1, P_2, ..., P_n\\} \\)\n\\Ensure The program \\( P^* \\) with the highest aggregated similarity\n\n\\For{each \\( P_i \\) in \\( P \\)}\n    \\For{each \\( P_j \\) in \\( P \\) where \\( i \\neq j \\)}\n        \\State \\( \\text{similarity}[i][j] \\gets \\text{Compute\\_Similarity}(P_i, P_j) \\)\n    \\EndFor\n\\EndFor\n\n\\For{each \\( P_i \\) in \\( P \\)}\n    \\State \\( \\text{agg\\_sim}[i] \\gets 0 \\)\n    \\For{each \\( P_j \\) in \\( P \\) where \\( i \\neq j \\)}\n        \\State \\( \\text{agg\\_sim}[i] \\gets \\text{agg\\_sim}[i] + \\text{similarity}[i][j] \\)\n    \\EndFor\n\\EndFor\n\n\\State \\( \\text{best\\_programs} \\gets \\{ P_i \\mid \\text{agg\\_sim}[i] = \\max(\\text{agg\\_sim}) \\} \\)\n\n\\State \\( P^* \\gets tie\\_breaking(\\text{best\\_programs}) \\)\n\n\\State \\Return \\( P^* \\)\n\n\\end{algorithmic}\n\\end{algorithm}",
        "trans_content": "\\begin{algorithm}[t!]\n<PLACEHOLDER_CAP_4>\n\\label{alg:ensemble_selection}\n\\begin{algorithmic}[1]\n\n\\Require List of programs \\( P = \\{P_1, P_2, ..., P_n\\} \\)\n\\Ensure The program \\( P^* \\) with the highest aggregated similarity\n\n\\For{each \\( P_i \\) in \\( P \\)}\n    \\For{each \\( P_j \\) in \\( P \\) where \\( i \\neq j \\)}\n        \\State \\( \\text{similarity}[i][j] \\gets \\text{Compute\\_Similarity}(P_i, P_j) \\)\n    \\EndFor\n\\EndFor\n\n\\For{each \\( P_i \\) in \\( P \\)}\n    \\State \\( \\text{agg\\_sim}[i] \\gets 0 \\)\n    \\For{each \\( P_j \\) in \\( P \\) where \\( i \\neq j \\)}\n        \\State \\( \\text{agg\\_sim}[i] \\gets \\text{agg\\_sim}[i] + \\text{similarity}[i][j] \\)\n    \\EndFor\n\\EndFor\n\n\\State \\( \\text{best\\_programs} \\gets \\{ P_i \\mid \\text{agg\\_sim}[i] = \\max(\\text{agg\\_sim}) \\} \\)\n\n\\State \\( P^* \\gets tie\\_breaking(\\text{best\\_programs}) \\)\n\n\\State \\Return \\( P^* \\)\n\n\\end{algorithmic}\n\\end{algorithm}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_14>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item\n{\\textbf{RQ1: How does \\tool\\ perform compared to standalone LLMs?}}\n    \\\\ We measure functional correctness using pass@1 metrics and evaluate if \\tool\\ improves the overall success rate over standalone models.\n    \\item\n{\\textbf {RQ2: How do CodeBLEU and CrossHair individually contribute to the accuracy of \\tool?}}\n    \\\\ An ablation study is conducted to assess the independent impact of CodeBLEU (syntactical and semantical similarity) and CrossHair (behavioral equivalence testing) in selecting correct outputs.\n    \\item\n{\\textbf{RQ3: How much accuracy can be gained using only free LLMs?}}\n    \\\\ We analyze whether \\tool\\ can effectively leverage only open-source models (e.g., OpenChat, CodeLlama, DeepSeekCoder) while maintaining high accuracy.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}\n    \\item\n{\\textbf{RQ1: 与独立大型语言模型相比，\\tool\\ 的性能表现如何？}}\n    \\\\ 我们采用 pass@1 指标衡量功能正确性，并评估 \\tool\\ 是否比独立模型提高了整体成功率。\n    \\item\n{\\textbf{RQ2: CodeBLEU 与 CrossHair 如何分别影响 \\tool\\ 的准确率？}}\n    \\\\ 通过消融实验评估 CodeBLEU（语法与语义相似度）和 CrossHair（行为等价测试）在筛选正确输出时的独立贡献。\n    \\item\n{\\textbf{RQ3: 仅使用免费大型语言模型能获得多少准确率提升？}}\n    \\\\ 我们分析 \\tool\\ 能否在仅使用开源模型（如 OpenChat、CodeLlama、DeepSeekCoder）的情况下仍保持较高准确率。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_15>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item \\textbf{HumanEval} \\cite{humaneval} is a benchmark specifically designed for evaluating the functional correctness of LLM-generated code. It consists of 164 Python programming problems, each presented with a structured format that includes a natural language prompt, a function signature, and a set of hidden test cases used for verification. The problems span a variety of domains, including mathematical computations, string manipulations, data structures, and logical reasoning. Since the primary focus of HumanEval is on correctness rather than efficiency or complexity, the generated program is evaluated using the pass@k metric. This metric determines whether at least one of the top k generated solutions is functionally correct based on the provided test cases. Given its controlled setup and standardized evaluation framework, HumanEval has become a widely adopted benchmark in the automated code generation community.\n\n    \\item \\textbf{LiveCodeBench} \\cite{livecodebench} is a comprehensive benchmark designed to assess the capabilities of LLMs across multiple programming related tasks. The dataset consists of four distinct categories: Code Generation, Self-Repair, Test Output Prediction, and Code Execution. For our evaluation, we focus exclusively on the Code Generation subset, which contains 511 programming problems covering a wide range of coding scenarios. These problems are more diverse than those in HumanEval, as they include real-world coding tasks, API usage, algorithmic challenges, and data structure manipulations. Additionally, LiveCodeBench introduces more complex problem formulations, requiring models to not only generate syntactically correct code but also ensure proper API usage and logical coherence. Like HumanEval, pass@k is used as the primary evaluation metric, providing a robust way to measure functional correctness. The inclusion of this dataset ensures that \\tool\\ is tested on a broader range of coding challenges that more accurately reflect real-world software development scenarios.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}\n    \\item \\textbf{HumanEval} \\cite{humaneval} 是一个专门用于评估大语言模型生成代码功能正确性的基准测试。它包含164个Python编程问题，每个问题均以结构化格式呈现，包括自然语言提示、函数签名以及用于验证的隐藏测试用例集。这些问题涵盖数学计算、字符串操作、数据结构和逻辑推理等多个领域。由于HumanEval主要关注正确性而非效率或复杂度，生成程序采用pass@k指标进行评估。该指标通过提供的测试用例判断前k个生成方案中是否至少有一个功能正确。凭借其受控的实验设置和标准化评估框架，HumanEval已成为自动化代码生成领域广泛采用的基准测试。\n\n    \\item \\textbf{LiveCodeBench} \\cite{livecodebench} 是一个综合性基准测试，旨在评估大语言模型在多种编程相关任务中的能力。该数据集包含四个独立类别：代码生成、自我修复、测试输出预测和代码执行。在我们的评估中，我们仅关注代码生成子集，其中包含511个覆盖广泛编码场景的编程问题。这些问题比HumanEval更具多样性，包含现实世界的编码任务、API使用、算法挑战和数据结构操作。此外，LiveCodeBench引入了更复杂的问题表述，要求模型不仅要生成语法正确的代码，还需确保API的正确使用和逻辑一致性。与HumanEval类似，pass@k作为主要评估指标，为功能正确性提供了稳健的衡量方式。该数据集的纳入确保\\tool\\能在更广泛反映真实软件开发场景的编码挑战中得到测试。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_16>",
        "env_name": "itemize",
        "content": "\\begin{itemize}\n    \\item Proprietary LLMs: GPT-4o, GPT-4, GPT-3.5, CoPilot, Gemini\n    \\item Open-Source LLMs: OpenChat, CodeBERT, Llama 3.2, Qwen2, Codestral, Gemma 2, DeepSeekCoder, CodeLlama, DolphinCoder\n\\end{itemize}",
        "trans_content": "\\begin{itemize}\n    \\item 专有大语言模型：GPT-4o、GPT-4、GPT-3.5、CoPilot、Gemini\n    \\item 开源大语言模型：OpenChat、CodeBERT、Llama 3.2、Qwen2、Codestral、Gemma 2、DeepSeekCoder、CodeLlama、DolphinCoder\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_17>",
        "env_name": "table",
        "content": "\\begin{table}[t!]\n    \\centering\n    <PLACEHOLDER_CAP_5>\n    \\label{tab:elfcg_performance}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{LLM} & \\textbf{HumanEval (\\%)} & \\textbf{LiveCodeBench (\\%)} \\\\\n        \\hline\n        GPT-4o         & 83.5  & 43.4  \\\\\n        GPT-4          & 82.3  & 42.2  \\\\\n        GPT-3.5        & 76.8  & 39.1  \\\\\n        CoPilot        & 74.4  & 41.6  \\\\\n        OpenChat (F)      & 71.3  & 37.3  \\\\\n        CodeBert (F)      & 68.9  & 37.2  \\\\\n        Llama 3.2 (F)     & 68.9  & 36.7  \\\\\n        Gemini         & 66.5  & 36.1  \\\\\n        Qwen2 (F)         & 62.1  & 36.4  \\\\\n        Codestral (F)     & 59.2  & 37.2  \\\\\n        Gemma 2  (F)      & 52.1  & 31.3  \\\\\n        DeepSeekCoder (F)  & 50.6  & 25.6  \\\\\n        CodeLlama  (F)     & 42.7  & 23.4  \\\\\n        DolphinCoder (F)   & 34.8  & 22.2  \\\\\n        \\hline\n        \\textbf{\\tool\\ (All)} & \\textbf{90.2 (90.9)} & \\textbf{50.2 (53.8)} \\\\\n        \\hline\n        \\textbf{\\tool\\ (Top 5)} & \\textbf{87.2 (90.9)} & \\textbf{48.3 (53.8)} \\\\\n        \\hline\n        \\textbf{\\tool\\ (All Free)} & \\textbf{80.5 (83.2)} & \\textbf{41.6 (44.1)} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}",
        "trans_content": "\\begin{table}[t!]\n    \\centering\n    <PLACEHOLDER_CAP_5>\n    \\label{tab:elfcg_performance}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{LLM} & \\textbf{HumanEval (\\%)} & \\textbf{LiveCodeBench (\\%)} \\\\\n        \\hline\n        GPT-4o         & 83.5  & 43.4  \\\\\n        GPT-4          & 82.3  & 42.2  \\\\\n        GPT-3.5        & 76.8  & 39.1  \\\\\n        CoPilot        & 74.4  & 41.6  \\\\\n        OpenChat (F)      & 71.3  & 37.3  \\\\\n        CodeBert (F)      & 68.9  & 37.2  \\\\\n        Llama 3.2 (F)     & 68.9  & 36.7  \\\\\n        Gemini         & 66.5  & 36.1  \\\\\n        Qwen2 (F)         & 62.1  & 36.4  \\\\\n        Codestral (F)     & 59.2  & 37.2  \\\\\n        Gemma 2  (F)      & 52.1  & 31.3  \\\\\n        DeepSeekCoder (F)  & 50.6  & 25.6  \\\\\n        CodeLlama  (F)     & 42.7  & 23.4  \\\\\n        DolphinCoder (F)   & 34.8  & 22.2  \\\\\n        \\hline\n        \\textbf{\\tool\\ (All)} & \\textbf{90.2 (90.9)} & \\textbf{50.2 (53.8)} \\\\\n        \\hline\n        \\textbf{\\tool\\ (Top 5)} & \\textbf{87.2 (90.9)} & \\textbf{48.3 (53.8)} \\\\\n        \\hline\n        \\textbf{\\tool\\ (All Free)} & \\textbf{80.5 (83.2)} & \\textbf{41.6 (44.1)} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_18>",
        "env_name": "table",
        "content": "\\begin{table}[t!]\n    \\centering\n    <PLACEHOLDER_CAP_6>\n    \\label{tab:ablation_codebleu_crosshair}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{$\\lambda$} & \\textbf{HumanEval (\\%)} & \\textbf{LiveCodeBench (\\%)} \\\\\n        \\hline\n        1 & 85.9 & 49.5 \\\\\n        .75 & 87.2 & 49.5 \\\\\n        .5 & \\textbf{90.2} & \\textbf{50.2} \\\\\n        .25 & 89.6 & 48.4 \\\\\n        0 & 89.6 & 47.9 \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}",
        "trans_content": "\\begin{table}[t!]\n    \\centering\n    <PLACEHOLDER_CAP_6>\n    \\label{tab:ablation_codebleu_crosshair}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{$\\lambda$} & \\textbf{HumanEval (\\%)} & \\textbf{LiveCodeBench (\\%)} \\\\\n        \\hline\n        1 & 85.9 & 49.5 \\\\\n        .75 & 87.2 & 49.5 \\\\\n        .5 & \\textbf{90.2} & \\textbf{50.2} \\\\\n        .25 & 89.6 & 48.4 \\\\\n        0 & 89.6 & 47.9 \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}",
        "need_trans": false
    }
]